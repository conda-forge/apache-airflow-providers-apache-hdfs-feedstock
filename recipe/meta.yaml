{% set name = "apache-airflow-providers-apache-hdfs" %}
{% set version = "2.2.2" %}


package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/apache-airflow-providers-apache-hdfs-{{ version }}.tar.gz
  sha256: 7bb5fee7791b09a6c5c35e4b3e685c21661774a182dd2f5919220c5e69386229

build:
  number: 0
  noarch: python
  script: {{ PYTHON }} -m pip install . -vv

requirements:
  host:
    - python >=3.7
    - pip
    - setuptools
  run:
    - python >=3.7
    - apache-airflow >=2.1.0
    # conda python-hdfs package includes dependencies for avro, dataframe, and kerberos extras
    - python-hdfs >=2.0.4
    - snakebite-py3
test:
  imports:
    - airflow.providers.apache.hdfs
    - airflow.providers.apache.hdfs.hooks
  commands:
    - pip check
  requires:
    - pip

about:
  home: https://airflow.apache.org/
  summary: Provider for Hadoop Distributed File System (HDFS) and WebHDFS for Apache Airflow
  license: Apache-2.0
  license_family: Apache
  license_file: LICENSE
  doc_url: https://airflow.apache.org/docs/apache-airflow-providers-apache-hdfs/stable/index.html
  dev_url: https://github.com/apache/airflow/

extra:
  recipe-maintainers:
    - xylar
